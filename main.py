import os
import gc
import time
import json
import typer
import queue
import psutil
import resource
import random
import pickle
import logging
import itertools
import warnings
import subprocess
import numpy as np
import polars as pl
import pandas as pd
from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
import multiprocessing as mp

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')

import tensorflow as tf
from filelock import FileLock
from collections import Counter
from sklearn.utils import shuffle
from multiprocessing import Manager
from collections import defaultdict
from typing_extensions import Annotated
from sklearn.preprocessing import LabelBinarizer

from tensorflow.keras.models import load_model
from scripts.deduplicate import deduplication_parallel
from scripts.annotate_new_data import calculate_total_rows
from scripts.train_new_model import (
    DynamicPaddingDataGenerator,
    ont_read_annotator,
)
from scripts.visualize_annot import save_plots_to_pdf
from scripts.simulate_training_data import (
    generate_training_reads,
    load_acc_priors,
    validate_acc_against_iupac
)
from scripts.demultiplex import generate_demux_stats_pdf
from scripts.plot_read_len_distr import plot_read_len_distr
from scripts.trained_models import trained_models, seq_orders
from scripts.correct_barcodes import generate_barcodes_stats_pdf
from scripts.extract_annotated_seqs import extract_annotated_full_length_seqs
from scripts.annotate_new_data import (
    preprocess_sequences,
    annotate_new_data,
    model_predictions,
    estimate_average_read_length_from_bin,
)
from scripts.preprocess_reads import (
    parallel_preprocess_data,
    find_sequence_files,
    extract_and_bin_reads,
    convert_tsv_to_parquet,
)
from scripts.export_annotations import (
    post_process_reads,
    plot_read_n_cDNA_lengths,
)

app = typer.Typer(rich_markup_mode="rich")

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =========================
# available trained models
# =========================


@app.command()
def availableModels():
    trained_models()

# ===========================================
# extract reads, read_names from fasta file
# ===========================================


@app.command()
def preprocess(fasta_dir: str, output_dir: str,
               output_base_qual: bool = typer.Option(False, help=(
                   "Whether to output base quality scores"
                   )),
               chunk_size: int = typer.Option(100000, help=(
                   "Base chunk size, dynamically adjusts based on read length"
                   )),
               threads: int = typer.Option(12, help=(
                   "Number of CPU threads"
                   ))):

    os.system("mkdir -p " + output_dir + "/full_length_pp_fa")
    files_to_process = find_sequence_files(fasta_dir)

    start = time.time()
    if len(files_to_process) == 1:
        # If there is only one file, process it in a single thread
        logger.info(
            "Only one file to process. Sorting reads "
            "into bins without parallelization."
        )
        extract_and_bin_reads(
            files_to_process[0],
            chunk_size,
            output_dir + "/full_length_pp_fa",
            output_base_qual
        )
        os.system(f"rm {output_dir}/full_length_pp_fa/*.lock")

        logger.info("Converting tsv files into parquet")
        convert_tsv_to_parquet(f"{output_dir}/full_length_pp_fa",
                               row_group_size=1000000)
        logger.info("Preprocessing finished!!")
    else:
        # Process files in parallel
        logger.info(
            "Multiple raw files found. Sorting reads "
            "into bins with parallelization."
            )
        parallel_preprocess_data(
            files_to_process,
            output_dir + "/full_length_pp_fa",
            chunk_size,
            output_base_qual,
            num_workers=threads
        )
    usage = resource.getrusage(resource.RUSAGE_CHILDREN)
    max_rss_mb = (
        usage.ru_maxrss / 1024
        if os.uname().sysname == "Linux"
        else usage.ru_maxrss
    )
    logger.info(f"Peak memory usage: {max_rss_mb:.2f} MB")
    logger.info(f"Elapsed time: {time.time() - start:.2f} seconds")

# ==============================
# plot read length distribution
# ==============================


@app.command()
def readlengthDist(output_dir: str):

    start = time.time()
    os.makedirs(f"{output_dir}/plots", exist_ok=True)

    logger.info("Generating the read-length distribution plot")
    plot_read_len_distr(output_dir + "/full_length_pp_fa",
                        output_dir + "/plots")
    logger.info("Finished generating the read-length distribution plot")

    usage = resource.getrusage(resource.RUSAGE_CHILDREN)
    max_rss_mb = usage.ru_maxrss / 1024 if os.uname().sysname == "Linux" \
        else usage.ru_maxrss
    logger.info(f"Peak memory usage during alignment: {max_rss_mb:.2f} MB")
    logger.info(f"Elapsed time: {time.time() - start:.2f} seconds")

# ===========================================
# inspect selected reads for annotations
# ===========================================


# Return the associated Parquet file for the read name
def load_read_index(index_file_path, read_name):
    df = (
        pl.read_parquet(index_file_path)
        .filter(pl.col("ReadName") == read_name)
    )
    if df.is_empty():
        logger.warning(f"Read {read_name} not found in the index.")
        return None
    return df["ParquetFile"][0]


@app.command()
def visualize(output_dir: str,
              output_file: str = typer.Option(
                  "full_read_annots",
                  help=("""Output annotation file name.\n
                        Extension .pdf will be added automatically""")),
              model_name: str = typer.Option(
                  "10x3p_sc_ont_011",
                  help="""Base model name. Use the name of the model without any suffix.\n
                  For model-type CRF, _w_CRF will be added to the base model name"""),
              model_type: Annotated[
                  str, typer.Option(help="""
                                    [red]REG[/red] = [green]CNN-LSTM[/green]\n
                                    [red]CRF[/red] = [green]CNN-LSTM-CRF[/green]
                                    """)
                                    ] = "CRF",
              num_reads: int = typer.Option(
                  None,
                  help="Number of reads to randomly visualize from each Parquet file."
              ),
              read_names: str = typer.Option(
                  None,
                  help="Comma-separated list of read names to visualize"),
              threads: int = typer.Option(2, help=(
                   "Number of CPU threads"))):

    start = time.time()

    base_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(base_dir, "models")
    models_dir = os.path.abspath(models_dir)

    utils_dir = os.path.join(base_dir, "utils")
    utils_dir = os.path.abspath(utils_dir)

    seq_order, sequences, barcodes, UMIs = seq_orders(
        os.path.join(utils_dir, "seq_orders.tsv"),
        model_name
    )

    num_labels = len(seq_order)
    model_path_w_CRF = None

    if model_type == "REG":
        model_path = os.path.join(models_dir, model_name + ".h5")
        model = load_model(model_path)
        with open(os.path.join(models_dir, model_name + "_lbl_bin.pkl"), "rb") as f:
            label_binarizer = pickle.load(f)
    else:
        model_path_w_CRF = os.path.join(models_dir, model_name + "_w_CRF.h5")
        model_params_json_path = model_path_w_CRF.replace(".h5", "_params.json")
        with open(model_params_json_path) as f:
            raw_params = json.load(f)

        params = {
            k: (
                v.lower() == "true" if isinstance(v, str) and v.lower() in ["true", "false"]
                else int(v) if isinstance(v, str) and v.isdigit()
                else float(v) if isinstance(v, str) and v.replace('.', '', 1).isdigit()
                else v
                )
                for k, v in raw_params.items()
                }
        model = ont_read_annotator(
            vocab_size=params["vocab_size"],
            embedding_dim=params["embedding_dim"],
            num_labels=num_labels,
            conv_layers=params["conv_layers"],
            conv_filters=params["conv_filters"],
            conv_kernel_size=params["conv_kernel_size"],
            lstm_layers=params["lstm_layers"],
            lstm_units=params["lstm_units"],
            bidirectional=params["bidirectional"],
            attention_heads=params["attention_heads"],
            dropout_rate=params["dropout_rate"],
            regularization=params["regularization"],
            learning_rate=params["learning_rate"],
            crf_layer=True  # Force CRF for this model
            )
        dummy_input = tf.zeros((1, 512), dtype=tf.int32)
        _ = model(dummy_input)
        model.load_weights(model_path_w_CRF)
        with open(os.path.join(models_dir,
                               model_name + "_w_CRF_lbl_bin.pkl"), "rb") as f:
            label_binarizer = pickle.load(f)

    palette = ['red', 'blue', 'green', 'purple', 'pink',
               'cyan', 'magenta', 'orange', 'brown']
    colors = {'random_s': 'black', 'random_e': 'black',
              'cDNA': 'gray', 'polyT': 'orange', 'polyA': 'orange'}

    i = 0
    for element in seq_order:
        if element not in ['random_s', 'random_e', 'cDNA', 'polyT', 'polyA']:
            colors[element] = palette[i % len(palette)]  # Cycle through the palette
            i += 1

    # Path to the read_index.parquet
    index_file_path = os.path.join(output_dir,
                                   "full_length_pp_fa/read_index.parquet")

    os.makedirs(f"{output_dir}/plots", exist_ok=True)

    folder_path = os.path.join(output_dir, "full_length_pp_fa")
    pdf_filename = f'{output_dir}/plots/{output_file}.pdf'

    if not num_reads and not read_names:
        logger.error("You must either provide a value for 'num_reads' or specify 'read_names'.")
        raise ValueError("You must either provide a value for 'num_reads' or specify 'read_names'.")

    selected_reads = []
    selected_read_names = []
    selected_read_lengths = []

    # If read_names are provided, visualize those specific reads
    if read_names:
        read_names_list = read_names.split(",")
        missing_reads = []

        for read_name in read_names_list:
            parquet_file = load_read_index(index_file_path, read_name)

            if parquet_file:
                parquet_path = os.path.abspath(parquet_file)

                try:
                    # Load the appropriate Parquet file and retrieve the read
                    df = pl.read_parquet(parquet_path).filter(pl.col("ReadName") == read_name)
                    if not df.is_empty():
                        read_seq = df["read"][0]
                        read_length = df["read_length"][0]
                        selected_reads.append(read_seq)
                        selected_read_names.append(read_name)
                        selected_read_lengths.append(read_length)
                except Exception as e:
                    logger.error(f"Error reading {parquet_path}: {e}")
            else:
                missing_reads.append(read_name)

        if missing_reads:
            logger.warning(f"The following reads were not found in the index: {', '.join(missing_reads)}")

    # If num_reads is provided, randomly select num_reads reads from the index
    elif num_reads:
        df_index = pl.read_parquet(index_file_path)
        all_read_names = df_index["ReadName"].to_list()
        selected_read_names = random.sample(all_read_names,
                                            min(num_reads, len(all_read_names)))

        for read_name in selected_read_names:
            parquet_file = load_read_index(index_file_path, read_name)

            if parquet_file:
                parquet_path = os.path.abspath(parquet_file)

                try:
                    df = pl.read_parquet(parquet_path).filter(pl.col("ReadName") == read_name)
                    if not df.is_empty():
                        read_seq = df["read"][0]
                        read_length = df["read_length"][0]
                        selected_reads.append(read_seq)
                        selected_read_lengths.append(read_length)
                except Exception as e:
                    logger.error(f"Error reading {parquet_path}: {e}")

    # Check if there are any selected reads to process
    if not selected_reads:
        logger.warning("No reads were selected. Skipping inference.")
        return

    # Perform annotation and plotting
    encoded_data = preprocess_sequences(selected_reads)
    predictions = annotate_new_data(encoded_data, model)
    annotated_reads = extract_annotated_full_length_seqs(
            selected_reads, predictions, model_path_w_CRF,
            selected_read_lengths, label_binarizer, seq_order,
            barcodes, n_jobs=threads,
        )
    save_plots_to_pdf(selected_reads, annotated_reads, selected_read_names,
                      pdf_filename, colors, chars_per_line=150)

    usage = resource.getrusage(resource.RUSAGE_CHILDREN)
    max_rss_mb = usage.ru_maxrss / 1024 if os.uname().sysname == "Linux" else usage.ru_maxrss  # Linux gives KB
    logger.info(f"Peak memory usage: {max_rss_mb:.2f} MB")
    logger.info(f"Elapsed time: {time.time() - start:.2f} seconds")

# =========================
# Annotate all the reads
# =========================


@app.command()
def annotate_reads(
    output_dir: str,
    whitelist_file: str,
    output_fmt: str = typer.Option("fasta", help=(
        "output format for demultiplexed reads: fasta or fastq"
        )),
    model_name: str = typer.Option(
        "10x3p_sc_ont_011",
        help="""Base model name. Use the name of the model without any suffix.\n
        For model-type CRF, _w_CRF will be added to the base model name"""),
    model_type: Annotated[
        str, typer.Option(
            help="""
            [red]REG[/red] = [green]CNN-LSTM[/green]\n
            [red]CRF[/red] = [green]CNN-LSTM-CRF[/green]\n
            [red]HYB[/red] = [green]First pass with CNN-LSTM and second \n
            (of reads not qualifying validity filter) with CNN-LSTM-CRF[/green]
            """)
        ] = "HYB",
    strand: str = typer.Option("unstranded", help=(
        "Strand information of the reads: 3', 5', unstranded"
        )),
    chunk_size: int = typer.Option(
        100000, help=(
        "Base chunk size, dynamically adjusts based on read length"
        )),
    bc_lv_threshold: int = typer.Option(2, help="lv-distance threshold for barcode correction"),
    threads: int = typer.Option(12, help="Number of CPU threads for barcode correction and demultiplexing"),
    max_queue_size: int = typer.Option(3, help="Max number of Parquet files to queue for post-processing")
):
    start = time.time()

    base_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(base_dir, "models")
    models_dir = os.path.abspath(models_dir)

    utils_dir = os.path.join(base_dir, "utils")
    utils_dir = os.path.abspath(utils_dir)

    model_path_w_CRF = None
    model_path = None

    if model_type == "REG" or model_type == "HYB":
        model_path = f"{models_dir}/{model_name}.h5"
        with open(f"{models_dir}/{model_name}_lbl_bin.pkl", "rb") as f:
            label_binarizer = pickle.load(f)

    seq_order, sequences, barcodes, UMIs = seq_orders(f"{utils_dir}/seq_orders.tsv", model_name)
    whitelist_df = pd.read_csv(whitelist_file, sep='\t')
    num_labels = len(seq_order)

    base_folder_path = os.path.join(output_dir, "full_length_pp_fa")

    invalid_output_file = os.path.join(output_dir, "annotations_invalid.tsv")
    valid_output_file = os.path.join(output_dir, "annotations_valid.tsv")

    parquet_files = sorted(
        [os.path.join(base_folder_path, f) for f in os.listdir(base_folder_path)
         if f.endswith('.parquet') and not f.endswith('read_index.parquet')],
        key=lambda f: estimate_average_read_length_from_bin(os.path.basename(f).replace(".parquet", ""))
    )

    column_mapping = {barcode: barcode for barcode in barcodes}

    whitelist_dict = {
    "cell_ids": {
        idx + 1: "-".join(map(str, row.dropna().unique()))
        for idx, row in whitelist_df[list(column_mapping.values())].iterrows()
    },
    **{
        input_column: whitelist_df[whitelist_column].dropna().unique().tolist()
        for input_column, whitelist_column in column_mapping.items()
        }
    }
    manager = Manager()
    cumulative_barcodes_stats = manager.dict({barcode: {'count_data': manager.dict(),
                                                        'min_dist_data': manager.dict()} for barcode in column_mapping.keys()})
    match_type_counter = manager.dict()
    cell_id_counter = manager.dict()

    fasta_dir = os.path.join(output_dir, "demuxed_fasta")
    os.makedirs(fasta_dir, exist_ok=True)

    if output_fmt == "fastq":
        logger.info("Selected output format: FASTQ")
        demuxed_fasta = os.path.join(fasta_dir, "demuxed.fastq")
        ambiguous_fasta = os.path.join(fasta_dir, "ambiguous.fastq")
    elif output_fmt == "fasta":
        logger.info("Selected output format: FASTA")
        demuxed_fasta = os.path.join(fasta_dir, "demuxed.fasta")
        ambiguous_fasta = os.path.join(fasta_dir, "ambiguous.fasta")

    demuxed_fasta_lock = FileLock(demuxed_fasta + ".lock")
    ambiguous_fasta_lock = FileLock(ambiguous_fasta + ".lock")

    invalid_file_lock = FileLock(invalid_output_file + ".lock")
    valid_file_lock = FileLock(valid_output_file + ".lock")

    def post_process_worker(task_queue, output_fmt, count, header_track, result_queue):
        """Worker function for processing reads and returning results."""
        while True:
            try:
                item = task_queue.get(timeout=10)
                if item is None:
                    break

                parquet_file, chunk_idx, predictions, read_names, reads, read_lengths, base_qualities = item
                bin_name = os.path.basename(parquet_file).replace(".parquet", "")

                append = "w" if chunk_idx == 1 else "a"

                local_cumulative_stats = {barcode: {'count_data': {}, 
                                                    'min_dist_data': {}} for barcode in column_mapping.keys()}
                local_match_counter, local_cell_counter = defaultdict(int), defaultdict(int)

                checkpoint_file = os.path.join(output_dir, "annotation_checkpoint.txt")

                with header_track.get_lock():
                    add_header = header_track.value == 0

                result = post_process_reads(
                    reads, read_names, output_fmt, base_qualities, model_type, 
                    pass_num, model_path_w_CRF,
                    predictions, label_binarizer, local_cumulative_stats,
                    read_lengths, seq_order, add_header, bin_name, output_dir,
                    invalid_output_file, invalid_file_lock, valid_output_file,
                    valid_file_lock, barcodes, whitelist_df, whitelist_dict,
                    bc_lv_threshold, checkpoint_file, 1, local_match_counter,
                    local_cell_counter, demuxed_fasta, demuxed_fasta_lock,
                    ambiguous_fasta, ambiguous_fasta_lock, threads
                )

                if result:
                    local_cumulative_stats, local_match_counter, local_cell_counter = result
                    result_queue.put((local_cumulative_stats,
                                      local_match_counter,
                                      local_cell_counter,
                                      bin_name))
                else:
                    logging.warning(f"No result from post_process_reads in {bin_name}, chunk {chunk_idx}")

                with count.get_lock():
                    count.value += 1

                gc.collect()
            except queue.Empty:
                pass

    num_workers = min(threads, mp.cpu_count() - 1)
    max_queue_size = max(3, num_workers * 2)

    def run_prediction_pipeline(result_queue, workers, max_idle_time=60):
        idle_start = None
        while any(worker.is_alive() for worker in workers) or not result_queue.empty():
            try:
                result = result_queue.get(timeout=15)
                idle_start = None
                if result:
                    local_cumulative_stats, local_match_counter, local_cell_counter, bin_name = result
                    for key, value in local_match_counter.items():
                        match_type_counter[key] = match_type_counter.get(key, 0) + value
                    for key, value in local_cell_counter.items():
                        cell_id_counter[key] = cell_id_counter.get(key, 0) + value
                    for barcode in local_cumulative_stats.keys():
                        for stat in ["count_data", "min_dist_data"]:
                            for key, value in local_cumulative_stats[barcode][stat].items():
                                cumulative_barcodes_stats[barcode][stat][key]=cumulative_barcodes_stats[barcode][stat].get(key, 0) + value
            except queue.Empty:
                if idle_start is None:
                    idle_start = time.time()
                    logging.info("Result queue idle, waiting for worker results...")
                elif time.time() - idle_start > max_idle_time:
                    raise TimeoutError("Result queue timed out after no data for 60 seconds and no workers finished.")

    if model_type == "REG" or model_type == "HYB":
        task_queue = mp.Queue(maxsize=max_queue_size)
        result_queue = mp.Queue()
        count = mp.Value('i', 0)
        header_track = mp.Value('i', 0)

        pass_num = 1

        logging.info(f"[Memory] RSS: {psutil.Process().memory_info().rss / 1e6:.2f} MB")

        workers = [mp.Process(target=post_process_worker,
                              args=(task_queue, output_fmt, count,
                                    header_track,
                                    result_queue)) for _ in range(num_workers)]

        logging.info(f"Number of workers = {len(workers)}")

        for worker in workers:
            worker.start()

        # process all the reads with CNN-LSTM model first
        logger.info("Starting first pass with regular model on all the reads")
        for parquet_file in parquet_files:
            for item in model_predictions(parquet_file, 1,
                                          chunk_size, model_path,
                                          model_path_w_CRF,
                                          model_type,
                                          num_labels):
                task_queue.put(item)
                with header_track.get_lock():
                    header_track.value += 1
        logging.info(f"[Memory] RSS: {psutil.Process().memory_info().rss / 1e6:.2f} MB")

        for _ in range(threads):
            task_queue.put(None)

        logging.info(f"[Memory] RSS: {psutil.Process().memory_info().rss / 1e6:.2f} MB")

        run_prediction_pipeline(result_queue, workers)

        logging.info(f"[Memory] RSS: {psutil.Process().memory_info().rss / 1e6:.2f} MB")

        logger.info("Finished first pass with regular model on all the reads")

        for worker in workers:
            worker.join()

        logging.info(f"[Memory] RSS: {psutil.Process().memory_info().rss / 1e6:.2f} MB")

        model_path_w_CRF = f"{models_dir}/{model_name}_w_CRF.h5"

        if model_type == "HYB":
            tmp_invalid_dir = os.path.join(output_dir,
                                           "tmp_invalid_reads")

            convert_tsv_to_parquet(tmp_invalid_dir,
                                   row_group_size=1000000)
            invalid_parquet_files = sorted(
                [os.path.join(tmp_invalid_dir, f) for f in os.listdir(tmp_invalid_dir) if f.endswith('.parquet') and not f.endswith('read_index.parquet')],
                key=lambda f: estimate_average_read_length_from_bin(os.path.basename(f).replace(".parquet", ""))
            )

            with open(f"{models_dir}/{model_name}_w_CRF_lbl_bin.pkl", "rb") as f:
                label_binarizer = pickle.load(f)

            # if model type selcted is HYB, process the failed reads in step 1 with CNN-LSTM-CRF model
            pass_num = 2
            task_queue = mp.Queue(maxsize=max_queue_size)
            result_queue = mp.Queue()

            with count.get_lock():
                count.value = 0

            with header_track.get_lock():
                header_track.value = 0

            workers = [mp.Process(target=post_process_worker,
                                  args=(task_queue, output_fmt, count,
                                        header_track,
                                        result_queue)) for _ in range(num_workers)]

            for worker in workers:
                worker.start()

            logger.info("Starting second pass with CRF model on invalid reads")
            for invalid_parquet_file in invalid_parquet_files:
                if calculate_total_rows(invalid_parquet_file) >= 100:
                    for item in model_predictions(invalid_parquet_file, 1,
                                                  chunk_size, model_path,
                                                  model_path_w_CRF,
                                                  model_type, num_labels):
                        task_queue.put(item)
                        with header_track.get_lock():
                            header_track.value += 1

            for _ in range(threads):
                task_queue.put(None)

            run_prediction_pipeline(result_queue, workers)
            logger.info("Finished second pass with CRF model on invalid reads")

            for worker in workers:
                worker.join()

    if model_type == "CRF":
        # process all the reads with CNN-LSTM-CRF model
        model_path_w_CRF = f"{models_dir}/{model_name}_w_CRF.h5"

        with open(f"{models_dir}/{model_name}_w_CRF_lbl_bin.pkl", "rb") as f:
            label_binarizer = pickle.load(f)

        task_queue = mp.Queue(maxsize=max_queue_size)
        result_queue = mp.Queue()
        count = mp.Value('i', 0)
        header_track = mp.Value('i', 0)

        pass_num = 1

        workers = [mp.Process(target=post_process_worker,
                              args=(task_queue, output_fmt, count,
                                    header_track,
                                    result_queue)) for _ in range(num_workers)]

        for worker in workers:
            worker.start()

        logger.info("Starting first pass with CRF model on all the reads")
        for parquet_file in parquet_files:
            for item in model_predictions(parquet_file, 1,
                                          chunk_size, None,
                                          model_path_w_CRF,
                                          model_type, num_labels):
                task_queue.put(item)
                with header_track.get_lock():
                    header_track.value += 1

        for _ in range(threads):
            task_queue.put(None)

        run_prediction_pipeline(result_queue, workers)

        logger.info("Finished first pass with CRF model on all the reads")

        for worker in workers:
            worker.join()
            worker.close()

    cumulative_barcodes_stats = {k: {'count_data': dict(v['count_data']),
                                     'min_dist_data': dict(v['min_dist_data'])} for k, v in cumulative_barcodes_stats.items()}

    os.makedirs(f"{output_dir}/plots", exist_ok=True)

    logger.info("Generating barcode stats plots")
    generate_barcodes_stats_pdf(cumulative_barcodes_stats, list(column_mapping.keys()),
                                pdf_filename=f"{output_dir}/plots/barcode_plots.pdf")
    logger.info("Generated barcode stats plots")

    logger.info("Generating demux stats plots")
    generate_demux_stats_pdf(f"{output_dir}/plots/demux_plots.pdf",
                             f"{output_dir}/matchType_readCount.tsv",
                             f"{output_dir}/cellId_readCount.tsv",
                             match_type_counter, cell_id_counter)
    logger.info("Generated demux stats plots")

    if os.path.exists(f"{output_dir}/annotations_valid.tsv"):
        with open(f"{output_dir}/annotations_valid.tsv", 'r') as f:
            header = f.readline().strip().split('\t')
            dtypes = {col: pl.Utf8 for col in header if col != "read_length"}
            dtypes["read_length"] = pl.Int64

        df = pl.scan_csv(f"{output_dir}/annotations_valid.tsv",
                         separator='\t',
                         dtypes=dtypes)
        annotations_valid_parquet_file = f"{output_dir}/annotations_valid.parquet"
        logger.info("Converting annotations_valid.tsv")
        df.sink_parquet(annotations_valid_parquet_file,
                        compression="snappy",
                        row_group_size=chunk_size)
        logger.info("Converted annotations_valid.tsv to annotations_valid.parquet")
        os.system(f"rm {output_dir}/annotations_valid.tsv")
        os.system(f"rm {output_dir}/annotations_valid.tsv.lock")

        logger.info("Generating valid read length and cDNA length distribution plots")
        plot_read_n_cDNA_lengths(output_dir)
        logger.info("Generated valid read length and cDNA length distribution plots")
        del df
    else:
        logger.warning("annotations_valid.tsv not found. Skipping Parquet conversion.")

    if os.path.exists(f"{output_dir}/annotations_invalid.tsv"):
        logger.info("annotations_invalid.tsv found — proceeding with Parquet conversion")
        with open(f"{output_dir}/annotations_invalid.tsv", 'r') as f:
            header = f.readline().strip().split('\t')
        dtypes = {col: pl.Utf8 for col in header if col != "read_length"}
        dtypes["read_length"] = pl.Int64

        df = pl.scan_csv(f"{output_dir}/annotations_invalid.tsv",
                         separator='\t',
                         dtypes=dtypes)
        annotations_invalid_parquet_file = f"{output_dir}/annotations_invalid.parquet"
        logger.info("Converting annotations_invalid.tsv")
        df.sink_parquet(annotations_invalid_parquet_file,
                        compression="snappy",
                        row_group_size=chunk_size)
        logger.info("Converted annotations_invalid.tsv to annotations_invalid.parquet")
        os.system(f"rm {output_dir}/annotations_invalid.tsv")
        os.system(f"rm {output_dir}/annotations_invalid.tsv.lock")
        del df
    else:
        logger.warning("annotations_invalid.tsv not found. Skipping Parquet conversion.")

    if os.path.exists(f"{output_dir}/demuxed_fasta/demuxed.fasta.lock"):
        os.system(f"rm {output_dir}/demuxed_fasta/demuxed.fasta.lock")
    if os.path.exists(f"{output_dir}/demuxed_fasta/ambiguous.fasta.lock"):
        os.system(f"rm {output_dir}/demuxed_fasta/ambiguous.fasta.lock")
    if os.path.exists(f"{output_dir}/demuxed_fasta/demuxed.fastq.lock"):
        os.system(f"rm {output_dir}/demuxed_fasta/demuxed.fastq.lock")
    if os.path.exists(f"{output_dir}/demuxed_fasta/ambiguous.fastq.lock"):
        os.system(f"rm {output_dir}/demuxed_fasta/ambiguous.fastq.lock")

    if model_type == "HYB":
        os.system(f"rm -r {output_dir}/tmp_invalid_reads")

    usage = resource.getrusage(resource.RUSAGE_CHILDREN)
    max_rss_mb = usage.ru_maxrss / 1024 if os.uname().sysname == "Linux" else usage.ru_maxrss  # Linux gives KB
    logger.info(f"Peak memory usage during annotation/barcode correction/demuxing: {max_rss_mb:.2f} MB")
    logger.info(f"Elapsed time: {time.time() - start:.2f} seconds")

# ======================================
# align inserts to the reference genome
# ======================================


@app.command()
def align(
    input_dir: str,
    ref: str,
    output_dir: str,
    preset: str = typer.Option(None, help="minimap2 preset"),
    filt_flag: str = typer.Option("0", help=(
        "Flag for filtering out (-F in samtools) the reads. "
        "Default is 260, to filter out secondary alignments "
        "and unmapped reads."
        )),
    mapq: int = typer.Option(0, help=(
        "minimap mapq for the alignments to be included "
        "for the downstream analysis"
        )),
    threads: int = typer.Option(12, help="number of CPU threads"),
    add_minimap_args: str = typer.Option("", help=(
        "additional minimap2 arguments"
        ))
):

    start = time.time()

    fasta_file = os.path.join(input_dir, "demuxed_fasta/demuxed.fasta")

    os.makedirs(f'{output_dir}/aligned_files', exist_ok=True)
    output_bam_dir = os.path.join(output_dir, 'aligned_files')

    output_bam = os.path.join(output_bam_dir, "demuxed_aligned.bam")

    minimap2_cmd = f"minimap2 -t {threads} -ax {preset} {add_minimap_args} \
        {ref} {fasta_file} | samtools view -h -F {filt_flag} -q {mapq} \
            -@ {threads} | samtools sort -@ {threads} -o {output_bam}"

    logger.info("Aligning reads to the reference genome")
    subprocess.run(minimap2_cmd, shell=True, check=True)
    logger.info(f"Alignment completed and sorted BAM saved as {output_bam}")

    logger.info(f"Indexing {output_bam}")
    subprocess.run(f"samtools index {output_bam}", shell=True, check=True)
    logger.info("Indexing complete")

    usage = resource.getrusage(resource.RUSAGE_CHILDREN)
    max_rss_mb = usage.ru_maxrss / 1024 if os.uname().sysname == "Linux" else usage.ru_maxrss  # Linux gives KB
    logger.info(f"Peak memory usage during alignment: {max_rss_mb:.2f} MB")
    logger.info(f"Elapsed time: {time.time() - start:.2f} seconds")

# ==============================
# Deduplication using UMI-tools
# ==============================


@app.command()
def dedup(
    input_dir: str,
    lv_threshold: int = typer.Option(2, help=(
        "levenshtein distance threshold for UMI similarity"
        )),
    stranded: bool = typer.Option(True, help=(
        "if directional or non-directional library"
        )),
    per_cell: bool = typer.Option(True, help=(
        "whether to correct umi's per cell basis"
        )),
    threads: int = typer.Option(12, help="number of CPU threads")
):

    start = time.time()

    aligned_bam = "aligned_files/demuxed_aligned.bam"
    dup_marked_bam = "aligned_files/demuxed_aligned_dup_marked.bam"

    input_bam = os.path.join(input_dir, aligned_bam)
    out_bam = os.path.join(input_dir, dup_marked_bam)

    deduplication_parallel(input_bam, out_bam,
                           lv_threshold, per_cell,
                           threads, stranded)

    usage = resource.getrusage(resource.RUSAGE_CHILDREN)
    max_rss_mb = usage.ru_maxrss / 1024 if os.uname().sysname == "Linux" else usage.ru_maxrss  # Linux gives KB
    logger.info(f"Peak memory usage: {max_rss_mb:.2f} MB")
    logger.info(f"Elapsed time: {time.time() - start:.2f} seconds")

# ===========================
# Simulate training dataset
# ===========================


@app.command()
def simulate_data(model_name: str,
                  output_dir: str,
                  num_reads: int = typer.Option(50000, help="number of reads to simulate"),
                  mismatch_rate: float = typer.Option(0.05, help="mismatch rate"),
                  insertion_rate: float = typer.Option(0.05, help="insertion rate"),
                  deletion_rate: float = typer.Option(0.06, help="deletion rate"),
                  min_cDNA: int = typer.Option(100, help="minimum cDNA length"),
                  max_cDNA: int = typer.Option(500, help="maximum cDNA length"),
                  polyT_error_rate: float = typer.Option(0.02, help=(
                      "error rate within "
                      "polyT or polyA segments"
                      )),
                  max_insertions: float = typer.Option(1, help=(
                      "maximum number of allowed "
                      "insertions after a base"
                      )),
                  threads: int = typer.Option(2, help="number of CPU threads"),
                  rc: bool = typer.Option(True, help=(
                      "whether to include reverse complements of the reads in "
                      "the training data.\nFinal dataset "
                      "will contain twice the number of user-specified reads"
                      )),
                  transcriptome: str = typer.Option(None, help="transcriptome fasta file"),
                  invalid_fraction: float = typer.Option(0.3, help="fraction of invalid reads to generate")):

    reads = []
    labels = []

    length_range = (min_cDNA, max_cDNA)

    base_dir = os.path.dirname(os.path.abspath(__file__))

    utils_dir = os.path.join(base_dir, "utils")
    utils_dir = os.path.abspath(utils_dir)

    seq_order, sequences, barcodes, UMIs = seq_orders(
        f"{utils_dir}/training_seq_orders.tsv", model_name
        )
    seq_order_dict = {}

    # ========== NEW: Load ACC priors if available ==========
    logger.info("Checking for ACC priors...")
    acc_priors = load_acc_priors(utils_dir, model_name)
    
    # Validate ACC priors against the IUPAC definition if both exist
    if 'ACC' in seq_order and acc_priors is not None:
        acc_idx = seq_order.index('ACC')
        acc_pattern = sequences[acc_idx]
        
        acc_sequences, acc_frequencies = acc_priors
        valid_seqs, invalid_seqs = validate_acc_against_iupac(
            acc_sequences, acc_pattern
        )
        
        if invalid_seqs:
            logger.warning(
                f"⚠️  Found {len(invalid_seqs)} ACC sequences not matching "
                f"IUPAC pattern '{acc_pattern}': {invalid_seqs[:3]}"
            )
            logger.warning("Falling back to random IUPAC expansion for ACC")
            acc_priors = None
        else:
            logger.info(f"✅ Validated {len(valid_seqs)} ACC prior sequences")
            logger.info(f"   Using empirical ACC distribution from acc_priors.tsv")
    elif 'ACC' in seq_order and acc_priors is None:
        logger.info("ℹ️  No ACC priors found. Using uniform IUPAC expansion")
    # ======================================================

    if transcriptome:
        logger.info("Loading transcriptome fasta file")
        transcriptome_records = list(SeqIO.parse(transcriptome, "fasta"))
        logger.info("Transcriptome fasta loaded")
    else:
        logger.info("No transcriptome provided. Will generate random transcripts...")
        transcriptome_records = []
        for i in range(num_reads):
            length = random.randint(min_cDNA, max_cDNA)
            seq_str = "".join(np.random.choice(list("ATCG")) for _ in range(length))
            record = SeqRecord(
                Seq(seq_str),
                id=f"random_transcript_{i+1}",
                description=f"Synthetic transcript {i+1}"
            )
            transcriptome_records.append(record)
        logger.info(f"Generated {len(transcriptome_records)} synthetic transcripts")

    for i in range(len(seq_order)):
        seq_order_dict[seq_order[i]] = sequences[i]

    training_segment_order = ["cDNA"]
    training_segment_order.extend(seq_order)
    training_segment_order.append("cDNA")

    training_segment_pattern = ["RN"]
    training_segment_pattern.extend(sequences)
    training_segment_pattern.append("RN")

    logger.info("Generating reads")
    reads, labels = generate_training_reads(num_reads, mismatch_rate,
                                            insertion_rate, deletion_rate,
                                            polyT_error_rate, max_insertions,
                                            training_segment_order,
                                            training_segment_pattern,
                                            length_range, threads, rc,
                                            transcriptome_records,
                                            invalid_fraction,
                                            acc_priors=acc_priors)  # Pass ACC priors
    logger.info("Finished generating reads")

    # ========== NEW: Report ACC distribution if used ==========
    if 'ACC' in seq_order and acc_priors is not None:
        from collections import Counter
        
        # Extract ACC sequences from generated data
        acc_sequences_generated = []
        for read, label in zip(reads, labels):
            acc_positions = [i for i, l in enumerate(label) if l == 'ACC']
            if acc_positions:
                acc_start = min(acc_positions)
                acc_end = max(acc_positions) + 1
                acc_seq = read[acc_start:acc_end]
                acc_sequences_generated.append(acc_seq)
        
        if acc_sequences_generated:
            acc_counts = Counter(acc_sequences_generated)
            logger.info(f"\n{'='*60}")
            logger.info("Generated ACC Distribution (Top 10):")
            logger.info(f"{'='*60}")
            for seq, count in acc_counts.most_common(10):
                freq = count / len(acc_sequences_generated)
                logger.info(f"  {seq}: {count:>6} ({freq*100:>5.2f}%)")
            logger.info(f"{'='*60}\n")
    # =========================================================

    os.makedirs(f'{output_dir}/simulated_data', exist_ok=True)

    logger.info("Saving the outputs")
    with open(f'{output_dir}/simulated_data/reads.pkl', 'wb') as reads_pkl:
        pickle.dump(reads, reads_pkl)
    with open(f'{output_dir}/simulated_data/labels.pkl', 'wb') as labels_pkl:
        pickle.dump(labels, labels_pkl)
    logger.info("Outputs saved")

# ===============
#  Train model
# ===============


@app.command()
def train_model(model_name: str,
                output_dir: str,
                num_val_reads: int = typer.Option(20, help="number of reads to simulate"),
                mismatch_rate: float = typer.Option(0.05, help="mismatch rate"),
                insertion_rate: float = typer.Option(0.05, help="insertion rate"),
                deletion_rate: float = typer.Option(0.06, help="deletion rate"),
                min_cDNA: int = typer.Option(100, help="minimum cDNA length"),
                max_cDNA: int = typer.Option(500, help="maximum cDNA length"),
                polyT_error_rate: float = typer.Option(0.02, help="error rate within polyT or polyA segments"),
                max_insertions: float = typer.Option(2, help="maximum number of allowed insertions after a base"),
                threads: int = typer.Option(2, help="number of CPU threads"),
                rc: bool = typer.Option(True, help=(
                    "whether to include reverse complements of "
                    "the reads in the training data.\nFinal dataset will "
                    "contain twice the number of user-specified reads"
                    )),
                transcriptome: str = typer.Option(None, help="transcriptome fasta file"),
                invalid_fraction: float = typer.Option(0.3, help="fraction of invalid reads to generate")):

    base_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(base_dir, "models")
    models_dir = os.path.abspath(models_dir)

    utils_dir = os.path.join(base_dir, "utils")
    utils_dir = os.path.abspath(utils_dir)

    param_file = f'{utils_dir}/training_params.tsv'

    with open(f"{output_dir}/simulated_data/reads.pkl", "rb") as r:
        reads = pickle.load(r)

    with open(f"{output_dir}/simulated_data/labels.pkl", "rb") as l:
        labels = pickle.load(l)

    df = pd.read_csv(param_file, sep="\t")

    # Ensure the model exists in the file
    if model_name not in df.columns:
        logger.info(f"{model_name} not found in the parameter file.")
        return

    logger.info(f"Extracting parameters for {model_name}")

    # Convert the model's parameter column to a dictionary of lists
    param_dict = {
        df.iloc[i, 0]: df.iloc[i][model_name].split(",") for i in range(len(df))
    }

    # Generate all possible combinations of parameters for this model
    param_combinations = list(itertools.product(*param_dict.values()))

    length_range = (min_cDNA, max_cDNA)

    seq_order, sequences, barcodes, UMIs = seq_orders(
        f"{utils_dir}/training_seq_orders.tsv", model_name
        )

    print(f"seq orders: {seq_order}")

    # ========== NEW: Load ACC priors for validation data ==========
    logger.info("Checking for ACC priors for validation data generation...")
    acc_priors = load_acc_priors(utils_dir, model_name)
    
    # Validate ACC priors against the IUPAC definition if both exist
    if 'ACC' in seq_order and acc_priors is not None:
        acc_idx = seq_order.index('ACC')
        acc_pattern = sequences[acc_idx]
        
        acc_sequences, acc_frequencies = acc_priors
        valid_seqs, invalid_seqs = validate_acc_against_iupac(
            acc_sequences, acc_pattern
        )
        
        if invalid_seqs:
            logger.warning(
                f"⚠️  Found {len(invalid_seqs)} ACC sequences not matching "
                f"IUPAC pattern '{acc_pattern}'"
            )
            logger.warning("Falling back to random IUPAC expansion for ACC in validation data")
            acc_priors = None
        else:
            logger.info(f"✅ Using ACC priors for validation data generation")
    elif 'ACC' in seq_order and acc_priors is None:
        logger.info("ℹ️  No ACC priors found. Using uniform IUPAC expansion for validation")
    # ===============================================================

    validation_segment_order = ["cDNA"]
    validation_segment_order.extend(seq_order)
    validation_segment_order.append("cDNA")

    validation_segment_pattern = ["RN"]
    validation_segment_pattern.extend(sequences)
    validation_segment_pattern.append("RN")

    if transcriptome:
        logger.info("Loading transcriptome fasta file")
        transcriptome_records = list(SeqIO.parse(transcriptome, "fasta"))
        logger.info("Transcriptome fasta loaded")
    else:
        logger.info("No transcriptome provided. Will generate random transcripts...")
        transcriptome_records = []
        for i in range(num_val_reads):
            length = random.randint(min_cDNA, max_cDNA)
            seq_str = "".join(np.random.choice(list("ATCG")) for _ in range(length))
            record = SeqRecord(
                Seq(seq_str),
                id=f"random_transcript_{i+1}",
                description=f"Synthetic transcript {i+1}"
            )
            transcriptome_records.append(record)
        logger.info(f"Generated {len(transcriptome_records)} synthetic transcripts")

    validation_reads, validation_labels = generate_training_reads(
        num_val_reads, mismatch_rate, insertion_rate, deletion_rate,
        polyT_error_rate, max_insertions, validation_segment_order,
        validation_segment_pattern, length_range, threads, rc,
        transcriptome_records, invalid_fraction,
        acc_priors=acc_priors)  # Pass ACC priors

    palette = ['red', 'blue', 'green', 'purple', 'pink',
               'cyan', 'magenta', 'orange', 'brown']
    colors = {'random_s': 'black', 'random_e': 'black', 'cDNA': 'gray',
              'polyT': 'orange', 'polyA': 'orange'}

    i = 0
    for element in seq_order:
        if element not in ['random_s', 'random_e', 'cDNA', 'polyT', 'polyA']:
            colors[element] = palette[i % len(palette)]  # Cycle through the palette
            i += 1

    validation_read_names = range(len(validation_reads))
    validation_read_lengths = []

    for validation_read in validation_reads:
        validation_read_lengths.append(len(validation_read))

    for idx, param_set in enumerate(param_combinations):
        model_filename = f"{model_name}_{idx}.h5"
        param_filename = f"{model_name}_{idx}_params.json"
        # curve_filename = f"{model_name}_{idx}_train_curve.png"

        os.makedirs(f'{output_dir}/{model_name}_{idx}', exist_ok=True)
        # Convert tuple to dictionary
        params = dict(zip(param_dict.keys(), param_set))

        # Extract model parameters
        batch_size = int(params.get("batch_size", 64))
        train_fraction = float(params.get("train_fraction", 0.80))
        vocab_size = int(params["vocab_size"])
        embedding_dim = int(params["embedding_dim"])
        num_labels = int(len(seq_order))
        conv_layers = int(params["conv_layers"])
        conv_filters = int(params["conv_filters"])
        conv_kernel_size = int(params["conv_kernel_size"])
        lstm_layers = int(params["lstm_layers"])
        lstm_units = int(params["lstm_units"])
        bidirectional = params["bidirectional"].lower() == "true"
        crf_layer = params["crf_layer"].lower() == "true"
        attention_heads = int(params["attention_heads"])
        dropout_rate = float(params["dropout_rate"])
        regularization = float(params["regularization"])
        learning_rate = float(params["learning_rate"])
        epochs = int(params["epochs"])

        logger.info(f"Training {model_filename} with parameters: {params}")

        # Save the parameters used in training
        os.makedirs(output_dir, exist_ok=True)
        with open(f"{output_dir}/{model_name}_{idx}/{param_filename}", "w") as param_file:
            json.dump(params, param_file, indent=4)

        # Shuffle data
        reads, labels = shuffle(reads, labels)

        unique_labels = list(set([item for sublist in labels for item in sublist]))
        label_binarizer = LabelBinarizer()

        label_binarizer.fit(unique_labels)

        # Save label binarizer
        os.makedirs(output_dir, exist_ok=True)
        with open(f"{output_dir}/{model_name}_{idx}/{model_name}_{idx}_lbl_bin.pkl", "wb") as lb_file:
            pickle.dump(label_binarizer, lb_file)

        # Train-validation split
        split_index = int(len(reads) * train_fraction)
        train_reads = reads[:split_index]
        train_labels = labels[:split_index]
        val_reads = reads[split_index:]
        val_labels = labels[split_index:]

        logger.info(f"Training reads: {len(train_reads)}, Validation reads: {len(val_reads)}")
        logger.info(f"Training Label Distribution: {Counter([label for seq in train_labels for label in seq])}")
        logger.info(f"Validation Label Distribution: {Counter([label for seq in val_labels for label in seq])}")

        # Data generators
        train_gen = DynamicPaddingDataGenerator(train_reads, train_labels,
                                                batch_size, label_binarizer)
        val_gen = DynamicPaddingDataGenerator(val_reads, val_labels,
                                              batch_size, label_binarizer)

        # Multi-GPU strategy
        strategy = tf.distribute.MirroredStrategy()
        logger.info(f"Number of devices: {strategy.num_replicas_in_sync}")

        with strategy.scope():
            model = ont_read_annotator(
                vocab_size, embedding_dim, num_labels,
                conv_layers=conv_layers, conv_filters=conv_filters,
                conv_kernel_size=conv_kernel_size,
                lstm_layers=lstm_layers, lstm_units=lstm_units,
                bidirectional=bidirectional, crf_layer=crf_layer,
                attention_heads=attention_heads, dropout_rate=dropout_rate,
                regularization=regularization, learning_rate=learning_rate
            )

        logger.info(f"Training {model_name}_{idx} with parameters: {params}")

        if crf_layer:
            dummy_input = tf.zeros((1, 512), dtype=tf.int32)  # Batch of 1, sequence length 512
            _ = model(dummy_input)

            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_val_accuracy',
                factor=0.5, patience=1,
                min_lr=1e-5, mode='max'
            )
            early_stopping = tf.keras.callbacks.EarlyStopping(
                monitor="val_loss_val", patience=1, restore_best_weights=True
                )

            history = model.fit(
                train_gen, validation_data=val_gen,
                epochs=epochs, callbacks=[early_stopping, reduce_lr]
                )
            model.save_weights(f"{output_dir}/{model_name}_{idx}/{model_name}_{idx}.h5")
        else:
            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_accuracy', factor=0.5, patience=1,
                min_lr=1e-5, mode='max'
                )
            early_stopping = tf.keras.callbacks.EarlyStopping(
                monitor="val_loss", patience=3, restore_best_weights=True
            )

            history = model.fit(
                train_gen, validation_data=val_gen,
                epochs=epochs, callbacks=[early_stopping, reduce_lr]
            )
            model.save(f"{output_dir}/{model_name}_{idx}/{model_filename}")

        history_df = pd.DataFrame(history.history)
        history_df.to_csv(f"{output_dir}/{model_name}_{idx}/{model_name}_{idx}_history.tsv",
                          sep='\t', index=False)

        encoded_data = preprocess_sequences(validation_reads)
        predictions = annotate_new_data(encoded_data, model)
        annotated_reads = extract_annotated_full_length_seqs(
            validation_reads, predictions, crf_layer,
            validation_read_lengths, label_binarizer,
            seq_order, barcodes, n_jobs=1
        )
        save_plots_to_pdf(validation_reads, annotated_reads,
                          validation_read_names,
                          f'{output_dir}/{model_name}_{idx}/{model_name}_{idx}_val_viz.pdf',
                          colors, chars_per_line=150)
        gc.collect()


if __name__ == "__main__":
    app()
